{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMOJVtT-FhJ"
      },
      "source": [
        "# Run an LLM with Ollama server in a Colab Notebook\n",
        "\n",
        "This Colab notebook demonstrates how to easily run an LLM using Ollama.  We’ll set up an Ollama server within Colab, allowing you to interact with powerful language models directly from your browser.\n",
        "\n",
        "To efficiently run large language models (LLMs), leveraging the power of Ollama servers within Google Colab Notebooks is a practical approach. This setup combines Ollama's computational capabilities with Colab’s accessible cloud environment, allowing users to execute advanced AI tasks directly from their browsers without needing local resources.\n",
        "\n",
        "Let’s get started!\n",
        "\n",
        "By: Sebastian Bassi [DNALinux.com](https://dnalinux.com)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Note**: If you are seeing this notebook in GitHub or in a non-colab server, press the following button to run it in Colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sbassi/py4bio/blob/master/Ollama_notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_sLrQaKCxX-"
      },
      "source": [
        "# Preparation work (install dependencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-DetMeD6YhZZ"
      },
      "outputs": [],
      "source": [
        "!apt install pciutils lshw\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxMPaXdNC_FG"
      },
      "source": [
        "# Start the LLM server (Ollama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5CX-b_9gZDfC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OBwT7bAHxt9"
      },
      "source": [
        "# Download an LLM model\n",
        "\n",
        "\n",
        "Your choice of LLM model depends on your GPU's capabilities and your tolerance for wait times.\n",
        "\n",
        "## Key Considerations\n",
        "\n",
        "Larger models (e.g., **Llama4** or **Llama3.3:70B**) require significant GPU RAM (over 50 GB). On platforms like Google Colab, this is only feasible with an A100 GPU. Don't use T4 GPUs for these models, as they are incompatible.\n",
        "\n",
        "Speed vs. Quality: Larger models deliver higher-quality outputs but respond slower. Smaller models are faster but produce less accurate results.\n",
        "\n",
        "Balance: For a middle ground between speed, resource usage, and output quality, consider **Gemma:12B** or **Llama3.1:8B**.\n",
        "\n",
        "T4 Compatibility: **Phi4** offers strong performance and, despite its size, is compatible with T4 GPUs.\n",
        "\n",
        "Don't use V2 TPU on Google Colab since it is slow for most tasks from the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "WLxm5xViZEnc"
      },
      "outputs": [],
      "source": [
        "# @title Select models to download\n",
        "import subprocess\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown Select a LLM model:\n",
        "\n",
        "LLM_model = \"gemma3:4b\" # @param [\"phi4\", \"deepseek-r1:7b\", \"gemma3:4b\", \"gemma3:12b\", \"llama4\", \"llama3.3:70b\", \"llama3.2:3b\", \"llama3.1:8b\"]\n",
        "!ollama pull {LLM_model}\n",
        "# @markdown ---\n",
        "# @markdown ### For more information on available models, check [Ollama](https://ollama.com/search)\n",
        "\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoTZG6ZypgpQ"
      },
      "source": [
        "Next cell reads all the PDF files in the **data_dir** directory, and it is processed into a Vector database named **db_name** and located at **db_dir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsyJfyWsVVWj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run a query against the LLM\n",
        "# @markdown Select the model and enter the prompt\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Select a model:\n",
        "LLM_model = \"gemma3:4b\" # @param [\"phi4\", \"gemma3:4b\", \"gemma3:12b\", \"llama4\", \"llama3.3:70b\", \"llama3.2:3b\", \"llama3.1:8b\"]\n",
        "# @markdown ### Enter a prompt:\n",
        "query = \"shorter version of this: In the rapidly evolving landscape of artificial intelligence, leveraging large language models has become integral to a myriad of applications. Running these sophisticated models efficiently and effectively is crucial for researchers, developers, and enthusiasts alike. One powerful yet accessible way to achieve this is by utilizing the Ollama server within Google Colab Notebooks. This combination offers an ideal platform that merges the computational power and versatility of Ollama with the ease of access provided by Colab’s cloud-based environment. In this guide, we will explore how you can set up and run an LLM using the Ollama server in a Colab Notebook, enabling you to harness advanced AI capabilities directly from your browser without any local infrastructure requirements. Whether for experimentation, research, or practical applications, mastering this setup is a valuable skill that opens up new possibilities in the realm of artificial intelligence. Let's dive into the process and unlock the potential of LLMs with Ollama and Colab!\" # @param {type:\"string\"}\n",
        "escaped_input = query.replace(\"'\", \"\\\\'\")\n",
        "# @markdown ---\n",
        "\n",
        "!ollama run {LLM_model} {escaped_input}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke0eC4FJb-x0"
      },
      "source": [
        "# Advanced options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qKuESu-2cOcT"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "\n",
        "# @title Adjust model parameters\n",
        "# @markdown To adjust model parameter in Ollama, you need to create a derived model. This cell will generate a derived model.\n",
        "\n",
        "# @markdown **Warning**: Not all model support all parameter. Using wrong parameters may generate a degraded model.\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Select a model to change (input model):\n",
        "LLM_model = \"gemma3:4b\" # @param [\"phi4\", \"gemma3:4b\", \"gemma3:12b\", \"llama4\", \"llama3.3:70b\", \"llama3.2:3b\", \"llama3.1:8b\"]\n",
        "# @markdown Enter new model name:\n",
        "new_model = \"genma3T07\" # @param {type:\"string\"}\n",
        "# @markdown Enter new template file name (if blank will use a random name):\n",
        "tpl_fn = \"\" # @param {type:\"string\",\"placeholder\":\"Modelfile\"}\n",
        "\n",
        "if tpl_fn == \"\":\n",
        "    # make a random filename\n",
        "    tpl_fn = tempfile.mktemp(suffix='.txt')\n",
        "\n",
        "# @markdown Temperature:  Increasing the temperature will make the model answer more creatively.\n",
        "temp = 0.7 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.05}\n",
        "# @markdown num_ctx: Sets the size of the context window used to generate the next token\n",
        "num_ctx = 4018 # @param {\"type\":\"slider\",\"min\":1024,\"max\":18000,\"step\":1}\n",
        "# @markdown seed: Sets the size of the context window used to generate the next token\n",
        "seed = 4482 # @param {\"type\":\"slider\",\"min\":0,\"max\":10000,\"step\":1}\n",
        "# @markdown Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)\n",
        "num_predict = 4790 # @param {\"type\":\"slider\",\"min\":-1,\"max\":10000,\"step\":1}\n",
        "# @markdown Top K: Reduces the probability of generating nonsense. A higher value will give more diverse answers, while a lower value will be more conservative.\n",
        "top_k = 20 # @param {\"type\":\"slider\",\"min\":1,\"max\":100,\"step\":1}\n",
        "# @markdown Top P: Works together with top-k. A higher value will lead to more diverse text, while a lower value will generate more focused and conservative text.\n",
        "top_p = 0.47 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "# @markdown Min P: Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out.\n",
        "min_p = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown [More about model parameters](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)\n",
        "\n",
        "# @markdown ---\n",
        "model_file = f\"\"\"FROM {LLM_model}\n",
        "PARAMETER temperature {temp}\n",
        "PARAMETER num_ctx {num_ctx}\n",
        "PARAMETER seed {seed}\n",
        "PARAMETER num_predict {num_predict}\n",
        "PARAMETER top_k {top_k}\n",
        "PARAMETER top_p {top_p}\n",
        "PARAMETER min_p {min_p}\n",
        "\"\"\"\n",
        "\n",
        "#mdir = \"/content/miniforge3/envs/ml/lib/python3.10/site-packages/ollama/models/\"\n",
        "\n",
        "with open(f\"{tpl_fn}\", \"w\") as f:\n",
        "    f.write(model_file)\n",
        "\n",
        "\n",
        "!ollama create {new_model} -f {tpl_fn}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cxMPaXdNC_FG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}