{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction & Data Base Embeddings Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = '/home/tagore/repos/ai/data/raw/raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extractor: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import math\n",
    "import hashlib\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "class TextExtractor:\n",
    "    def __init__(self, output_dir: str, input_dir: str, chroma_db_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.chroma_db_path = Path(chroma_db_dir)\n",
    "\n",
    "        # Ensure directories exist\n",
    "        self.input_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.chroma_db_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_pdf(self) -> list:\n",
    "        pdf_file_paths = []\n",
    "        for root, _, files in os.walk(self.input_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    pdf_file_paths.append(Path(root) / file)\n",
    "        return pdf_file_paths\n",
    "    \n",
    "    def get_html(self) -> list:\n",
    "        html_file_paths = []\n",
    "        for root, _, files in os.walk(self.input_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.html'):\n",
    "                    html_file_paths.append(Path(root) / file)\n",
    "        return html_file_paths\n",
    "    \n",
    "    def get_urls(self) -> list:\n",
    "        urls = []\n",
    "        with open(self.input_dir / 'urls.txt', 'r') as f:\n",
    "            urls = [line.strip() for line in f]\n",
    "        return urls\n",
    "    \n",
    "    def save_text_to_file(self, text: str, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves text to a file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "        - text (str): Text content to be saved.\n",
    "        - filename (str): Name of the file to be saved.\n",
    "        \"\"\"\n",
    "        output_file = self.output_dir / filename\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def save_a_pdf_text(self, pdf_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Extracts text from a PDF file and saves each page's text to a separate file,\n",
    "        ensuring a limited overlap between pages and avoiding pages with only one line of text.\n",
    "\n",
    "        Args:\n",
    "        - pdf_path (str): Path to the PDF file.\n",
    "        \"\"\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load_and_split()\n",
    "        \n",
    "        # Extract the source name and create a directory for it\n",
    "        source_name = Path(pdf_path).name.replace('.pdf', '')  # Remove the '.pdf' extension\n",
    "        source_dir = self.output_dir / source_name\n",
    "        source_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        previous_page_text = \"\"  # Initialize previous page text\n",
    "\n",
    "        for page_num, page_text in enumerate(pages, start=1):\n",
    "            current_page_text = page_text.page_content\n",
    "\n",
    "            # Check if the current page has more than one line of text\n",
    "            if len(current_page_text.splitlines()) > 1:\n",
    "                # Add a limited overlap from the previous page if it's not the first page\n",
    "                if page_num > 1:\n",
    "                    overlap = \"\\n\" + previous_page_text[-100:]  # Adjust the number of characters to overlap\n",
    "                    combined_text = overlap + current_page_text\n",
    "                else:\n",
    "                    combined_text = current_page_text\n",
    "                \n",
    "                filename = f\"{source_name}_page_{page_num}.txt\"\n",
    "                output_file = source_dir / filename  # Save the file in the source directory\n",
    "                self.save_text_to_file(combined_text, output_file)  # Pass the combined text for saving\n",
    "\n",
    "                print(f\"Saved page {page_num} to {output_file}\")\n",
    "            \n",
    "            previous_page_text = current_page_text  # Update previous page text for the next iteration\n",
    "\n",
    "    \n",
    "    def save_pdfs_texts(self, pdf_paths: list) -> None:\n",
    "        \"\"\"\n",
    "        Extracts text from a list of PDF files.\n",
    "\n",
    "        Args:\n",
    "        - pdf_paths (list): List of paths to PDF files.\n",
    "        \"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            self.save_a_pdf_text(pdf_path)\n",
    "\n",
    "    def delete_oneline_files(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes files that have only one line of text in the output directory and its subdirectories.\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(self.output_dir):\n",
    "            for file in files:\n",
    "                file_path = Path(root) / file  # Construct the full file path\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) == 1:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted {file_path}\")\n",
    "\n",
    "    def extract_pdf_texts(sef, pdf_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Extracts text from a PDF file and add them  all to a list,\n",
    "        ensuring a limited overlap between pages and avoiding pages with only one line of text.\n",
    "\n",
    "        Args:\n",
    "        - pdf_path (str): Path to the PDF file.\n",
    "        \"\"\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load_and_split()\n",
    "        result = []\n",
    "        previous_page_text = \"\"  # Initialize previous page text\n",
    "\n",
    "        for page_num, page_text in enumerate(pages, start=1):\n",
    "            current_page_text = page_text.page_content\n",
    "\n",
    "            # Check if the current page has more than one line of text\n",
    "            if len(current_page_text.splitlines()) > 1:\n",
    "                # Add a limited overlap from the previous page if it's not the first page\n",
    "                if page_num > 1:\n",
    "                    overlap = \"\\n\" + previous_page_text[-100:]  # Adjust the number of characters to overlap\n",
    "                    combined_text = overlap + current_page_text\n",
    "                else:\n",
    "                    combined_text = current_page_text\n",
    "\n",
    "                result.append(combined_text)\n",
    "\n",
    "            previous_page_text = current_page_text  # Update previous page text for the next iteration\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def save_a_html_text(self, html_path: str, chars_per_file: int = 1500, overlap: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Extracts text from a HTML file and saves it to multiple files with specified character limits and overlap.\n",
    "\n",
    "        Args:\n",
    "        - html_path (str): Path to the HTML file.\n",
    "        - chars_per_file (int): Number of characters per output file.\n",
    "        - overlap (int): Number of overlapping characters between consecutive files.\n",
    "        \"\"\"\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "            text = ' '.join(text.split())  # Remove extra spaces\n",
    "\n",
    "        # Extract the source name and create a directory for it\n",
    "        source_name = Path(html_path).stem  # Remove the file extension\n",
    "        source_dir = self.output_dir / source_name\n",
    "        source_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        start = 0\n",
    "        file_num = 1\n",
    "        while start < len(text):\n",
    "            end = start + chars_per_file\n",
    "            chunk = text[start:end]\n",
    "\n",
    "            # Add overlap if it's not the first file\n",
    "            if start > 0:\n",
    "                chunk = text[start - overlap:end]\n",
    "\n",
    "            filename = f\"{source_name}_part_{file_num}.txt\"\n",
    "            output_file = source_dir / filename\n",
    "            self.save_text_to_file(chunk, output_file)\n",
    "            print(f\"Saved part {file_num} to {output_file}\")\n",
    "\n",
    "            start += chars_per_file  # Move to the next chunk\n",
    "            file_num += 1\n",
    "\n",
    "    def save_html_texts(self, html_paths: list) -> None:\n",
    "        \"\"\"\n",
    "        Extracts text from a list of HTML files and saves them to multiple files.\n",
    "\n",
    "        Args:\n",
    "        - html_paths (list): List of paths to HTML files.\n",
    "        \"\"\"\n",
    "        for html_path in html_paths:\n",
    "            self.save_a_html_text(html_path)\n",
    "\n",
    "    def extract_html_text(self, html_path: str, chars_per_file: int = 1500, overlap: int = 100) -> list:\n",
    "        \"\"\"\n",
    "        Extracts text from a HTML file and returns it as a string.\n",
    "\n",
    "        Args:\n",
    "        - html_path (str): Path to the HTML file.\n",
    "\n",
    "        Returns:\n",
    "        - list: list of extracted text content.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "            text = ' '.join(text.split())\n",
    "\n",
    "        start = 0\n",
    "        file_num = 1\n",
    "        while start < len(text):\n",
    "            end = start + chars_per_file\n",
    "            chunk = text[start:end]\n",
    "\n",
    "            # Add overlap if it's not the first file\n",
    "            if start > 0:\n",
    "                chunk = text[start - overlap:end]\n",
    "\n",
    "            result.append(chunk)\n",
    "\n",
    "            start += chars_per_file\n",
    "\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extractor: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enter a title, displayed at the top of the window. Enter the main heading, usually the same as the t',\n",
       " 'usually the same as the title. Be bold in stating your key points. Put them in a list: The first item in your list The second',\n",
       " 'm in your list The second item; italicize key words Improve your image by including an image. Add a link to your favorite Web',\n",
       " 'link to your favorite Web site. Break up your page with a horizontal rule or two. Finally, link to another page in your own W',\n",
       " 'nother page in your own Web site. © Wiley Publishing, 2011 x']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = '/home/tagore/repos/ai/data/exampe_debug_folder'\n",
    "input = '/home/tagore/repos/ai/data/example_data'\n",
    "chroma_db = '/home/tagore/repos/ai/data/example_database'\n",
    "test_extractor = TextExtractor(output, input, chroma_db)\n",
    "\n",
    "pdfs = test_extractor.get_pdf()\n",
    "l = test_extractor.get_html()\n",
    "test_extractor.extract_html_text(l[0],100,25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
